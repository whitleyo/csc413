{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1.  Given the vocabulary size $V$ and embedding dimensionality $d$, how many parameters does the GLoVE model have?\n",
    "2.  Write the gradient of the loss function with respect to one parameter vector $\\mathbf{w}_i$.\n",
    "3.  Implement the gradient update of GLoVE.\n",
    "4.  Train the model with varying dimensionality $d$.\n",
    "Which $d$ leads to optimal validation performance?\n",
    "Why does / doesn't larger $d$ always lead to better validation error?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. V x d + V\n",
    "2. $$\\nabla_w \\textbf{L} = (\\sum_{j != i}^{V} 2*(w_i^\\intercal * w_j + b_i + b_j -log X_{ij} * )w_j) + 4(w_i^\\intercal * w_i + 2*b_i - log X_{ii})*w_i$$\n",
    "3. See code above in grad_GLoVE\n",
    "4. It seems that an optimum  is reached at about 10, after which validation error increases, then decreases again. It may be the case that overfitting starts occuring after a dimension of 10 is reached but this is compensated for by dimensions close to the vocabulary size effectively memorizing co-occurence relationships without losing information to compressing the data to a strongly reduced dimension. So after 10, the amount of info gained per dimension is not enough to compensate for overfitting to the dataset, while after about 200, the parameters may be coming close to effectively copying the co-occurence matrix. Given that the co-occurence matrix represents an estimate of expected values for co-occurence, provided large enough training and validation sets, the matrix values should be reasonably close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:\n",
    "Questions are abridged from text in handout\n",
    "\n",
    "   1. What is the total number of trainable parameters in the model? Which part of the model has the\n",
    "largest number of trainable parameters?\n",
    "  \n",
    "     $250*16$ params for $R$ (word to embedding matrix) = $4000$ params\n",
    "  \n",
    "     $16*3*128$ = $6144$ params for $W^{(2)}$ (matrix linking embeddings to logistic function that outputs hidden layer) \n",
    "     \n",
    "     $b^{(2)}$ (bias for logistic function outputting hidden layer) has $128$ params\n",
    "     \n",
    "     $W^{(3)}$ (matrix linking hidden layer to softmax) has $128*250$ = $32,000$ params\n",
    "     \n",
    "     $b^{(3)}$ (bias for softmax) has $250$ params\n",
    "     \n",
    "     By far, $W^{(3)}$ has most params. total params = $42522$\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "   2. If we wanted to use an n-gram model with the same context length as our network,\n",
    "weâ€™d need to store the counts of all possible 4-grams. If we stored all the counts explicitly,\n",
    "how many entries would this table have? \n",
    "\n",
    "250^4 = 10^9.59 params\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "\n",
    "Here's the output for check_gradient()\n",
    "\n",
    "![check_gradient_output](check_gradients_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:\n",
    "\n",
    "1. The model seems to give sensible predictions for 4-grams that don't appear in the dataset. Interestingly, for a very generic 3 gram ('if i even'), probabilities for the next word are more evenly spread out among possibilities than something perhaps more specific in subject matter ('the police state'). Experimenting with 'a police state', it appears that this 3 gram appeared in the dataset multiple times, so the results for 'the police state' are likely heavily dependent on whatever was learned for 'a police state', which would be likely be used in contexts similar to 'Some country is a police state.', whose sentences naturally end with a period.\n",
    "\n",
    "2. \n",
    "\n",
    "    Examples of clusters in TSNE of nn representation (weights): 1) he, she, they, I, you (pronouns) 2) if, and, but, or, because (conjunctions, logic). \n",
    "\n",
    "    ![tsne_nn](tsne_nn.png)\n",
    "\n",
    "    Examples of clusters in the TSNE of GloVE 250D representation: less, than, more, three, four, five, two. he, she, never, said. Results appear to be more conceptural\n",
    "\n",
    "    ![tsne_glove](tsne_glove.png)\n",
    "\n",
    "    Examples of clusters in TSNE of GloVE 2D representation: I, we, do, you, know. Terms appear unrelated\n",
    "\n",
    "    ![tsne_glove_2d](tsne_glove_2d.png)\n",
    "\n",
    "    2D Glove representation: some pairs of words near each other appear to be sensible, but there don't appear to be discrete clusters of words, save for a dense cloud of words in the upper right corner of the plot.\n",
    "\n",
    "    ![2d_glove](2d_glove.png)\n",
    "\n",
    "3. No they are not close in the representation. New appears to be close to adjectives, while York appears to be close to nouns or titles (which of course are followed by names, which form nouns with the titles). I would speculate that the represenation learned has encoded word function before conceptual similarity.\n",
    "\n",
    "4. Government and University are closer together. Again, I speculated that the representation learned has encoded word function before conceptual similarity/co-occurence. Government and University are nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
